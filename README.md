# Simple Chat Gateway with vLLM

Готовое решение для поднятия локального API генерации текста: FastAPI-шлюз с хранением сессий + vLLM, завернутые в Docker Compose. Все инструкции и примеры даны на русском языке с подробным описанием логики работы.

## Архитектура и потоки данных

- **vLLM** — OpenAI-совместимый сервер, который обслуживает модель из локальной папки `/app/model`. Модель не скачивается во время запуска контейнера.
- **FastAPI gateway** — тонкий слой над vLLM. Он принимает запросы клиентов, проверяет `vllm_api_key`, заводит сессии в памяти, поддерживает системный промпт и историю диалога, оценивает длину контекста локальным токенайзером и триммит историю, чтобы уложиться в лимиты модели.
- **CLI-клиент** (в каталоге `client`) — пример того, как пошагово работать с API: инициализация сессии, установка системного промпта, отправка сообщений. Работает через обычные HTTP-запросы и может служить эталоном для интеграции.

### Жизненный цикл сессии
1. Клиент вызывает `POST /init_session` и передаёт `vllm_api_key`.
2. Gateway обращается к vLLM для проверки ключа. Если ключ валиден, создаётся запись сессии в оперативной памяти со временем последней активности и пустой историей сообщений. В ответе возвращается `session_id` и оставшееся время жизни (`expires_in`).
3. По желанию клиент вызывает `POST /set_system_prompt`, чтобы задать системную инструкцию. Промпт сохраняется внутри состояния сессии и применяется к каждому последующему запросу.
4. При каждом `POST /chat` gateway:
   - добавляет новое пользовательское сообщение в историю;
   - оценивает количество токенов истории + текущего запроса локальным токенайзером;
   - удаляет самые старые пары сообщений, пока сумма токенов не станет меньше `MAX_CONTEXT_TOKENS` + запрашиваемый `max_tokens` ответа;
   - отправляет запрос в vLLM и возвращает `reply`, `finish_reason` и счётчики токенов;
   - обновляет таймстемп активности сессии.
5. Если сессия простаивает больше `SESSION_TTL_SECONDS`, фоновый очиститель удаляет её. Можно завершить работу вручную через `POST /stop_session`, передав `session_id`.

## Содержимое репозитория
- `app/main.py` — исходный код FastAPI-сервиса с логикой сессий, триммингом контекста и healthcheck.
- `Dockerfile` — сборка gateway.
- `Dockerfile.vllm` — CUDA-образ с предустановленным `vllm`.
- `docker-compose.example.yaml` — пример запуска двух контейнеров (gateway + vLLM). Пользователи копируют файл в `docker-compose.yaml` и подставляют свои значения.
- `requirements.txt` — зависимости gateway.
- `client/` — CLI-клиент и собственный README с пошаговыми инструкциями.

## Требования
- Docker + Docker Compose v2.
- GPU с поддержкой CUDA для vLLM (пример использует `deploy.resources.reservations.devices`; подберите значение под свой рантайм). Образ собирается на базе `pytorch/pytorch:2.9.1-cuda12.8-cudnn9-runtime` и ставит `vllm` (>=0.10.0).
- Локальная HF-совместимая модель, доступная на диске и смонтированная в контейнер vLLM по пути `./model -> /app/model`.

## Подготовка окружения

1. **Скачайте модель заранее.** Убедитесь, что в папке есть файлы токенайзера — gateway читает их при старте.
2. **Скопируйте compose-шаблон.**
   ```bash
   cp docker-compose.example.yaml docker-compose.yaml
   ```
3. **Создайте `.env` с переменными окружения** (минимальный пример):
   ```bash
   cat > .env <<'ENV'
   # Единый ключ, который знает vLLM и который передается в /init_session
   VLLM_API_KEY=INTERNAL_VLLM_KEY

   # Максимальная длина контекста модели (используется в gateway для тримминга истории)
   MAX_CONTEXT_TOKENS=8000

   # TTL сессии в секундах при отсутствии активности
   SESSION_TTL_SECONDS=600

   # Максимальная длина контекста, которую пробрасываем в vLLM
   VLLM_MAX_MODEL_LEN=8192
   ENV
   ```
4. **При необходимости отредактируйте `docker-compose.yaml`.** Укажите путь к модели, ресурсы GPU и порты.
5. **Запустите**:
   ```bash
   docker compose up -d --build
   ```
6. **Проверьте здоровье сервисов**:
   ```bash
   curl http://localhost:8080/health
   docker compose ps
   ```

## Переменные окружения

### Gateway
| Переменная | Назначение | Значение по умолчанию |
| --- | --- | --- |
| `VLLM_BASE_URL` | URL vLLM внутри сети Compose | `http://vllm:8000/v1` |
| `VLLM_MODEL_NAME` | Имя/путь модели для vLLM и токенайзера | `/app/model` |
| `MAX_CONTEXT_TOKENS` | Лимит входных токенов (история + запрос), gateway сам режет историю | `8000` |
| `SESSION_TTL_SECONDS` | TTL сессии без активности | `600` |

### vLLM
| Переменная | Назначение | Примечание |
| --- | --- | --- |
| `VLLM_API_KEY` | Должен совпадать с ключом, который клиенты передают в `/init_session` | Требуется для авторизации запросов |
| `VLLM_MAX_MODEL_LEN` | Пробрасывается в `--max-model-len` | Меняйте при необходимости |

> Все переменные задаются через `.env`, который автоматически подхватывает `docker compose`.

## Подробные примеры работы с API

Базовый URL шлюза: `http://localhost:8080`

### 1. Создать сессию
```bash
curl -X POST http://localhost:8080/init_session \
  -H 'Content-Type: application/json' \
  -d '{"vllm_api_key": "INTERNAL_VLLM_KEY"}'
```
Ответ вернёт `session_id` и `expires_in` в секундах.

### 2. Установить системный промпт (опционально)
```bash
curl -X POST http://localhost:8080/set_system_prompt \
  -H 'Content-Type: application/json' \
  -d '{"session_id": "<uuid>", "system_prompt": "Ты строгий ассистент по DL"}'
```

### 3. Отправить сообщение в чат
```bash
curl -X POST http://localhost:8080/chat \
  -H 'Content-Type: application/json' \
  -d '{"session_id": "<uuid>", "message": "В чём отличие vLLM от TGI?", "max_tokens": 256, "temperature": 0.3}'
```
Ответ содержит `reply`, `finish_reason`, `total_tokens`, `input_tokens`, `output_tokens`.

### 4. Завершить сессию вручную (по желанию)
```bash
curl -X POST http://localhost:8080/stop_session \
  -H 'Content-Type: application/json' \
  -d '{"session_id": "<uuid>"}'
```

### 5. Healthcheck
```bash
curl http://localhost:8080/health
```

## Типовые сценарии использования
- **Быстрая проверка модели.** Создайте сессию, задайте короткий системный промпт и отправьте несколько вопросов подряд — gateway сам управляет историей и ограничениями токенов.
- **Эксперименты с тонкой настройкой.** В теле `/chat` передавайте `extra` с дополнительными параметрами vLLM (например, top_p, repetition_penalty). Gateway проксирует эти поля в vLLM.
- **Интерактивная работа из терминала.** Используйте CLI-клиент из `client/README.md`: он демонстрирует полную последовательность вызовов API и работу с ошибками.

## Детали реализации
- **Сессии в памяти.** Очистка неактивных сессий каждые 60 секунд (`SESSION_TTL_SECONDS`).
- **Контекст.** Перед запросом к vLLM gateway оценивает токены с помощью локального токенайзера и удаляет самые старые пары сообщений, чтобы уложиться в `MAX_CONTEXT_TOKENS` + `max_tokens` ответа.
- **Системный промпт.** По умолчанию задаётся русскоязычный промпт; можно менять через `/set_system_prompt` в рамках сессии.
- **Авторизация.** Клиент на этапе `/init_session` отправляет `vllm_api_key`. Gateway проверяет его, делая запрос к vLLM, и выдаёт `session_id`. В дальнейших запросах используется только `session_id`; отдельного публичного токена нет.
- **Без внешних загрузок.** vLLM всегда читает модель из `/app/model`; скачивание из интернета не требуется и не производится.
- **CORS.** Включён на все источники, при необходимости ограничьте список в `app/main.py`.

## Обновление и обслуживание
- Для изменения параметров обновите `.env` и перезапустите `docker compose up -d`.
- Чтобы сменить модель, замените содержимое `./model` и перезапустите сервисы.
- Сессии не сохраняются между перезапусками gateway. Для персистентности интегрируйте внешний storage (Redis/Postgres).

## Отладка
- Посмотреть логи gateway: `docker compose logs -f gateway`
- Посмотреть логи vLLM: `docker compose logs -f vllm`
- Проверить здоровье сервисов: `docker compose ps`

## Дополнительные материалы
- Подробное руководство по CLI-клиенту и примерам работы: `client/README.md`.
- Шаблон конфигурации клиента: `client/chat.example.json`.

## Лицензия
Поставьте свою лицензию при необходимости; по умолчанию репозиторий без лицензии.
