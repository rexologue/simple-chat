# Simple Chat Gateway with vLLM

Этот репозиторий разворачивает готовый стек из двух контейнеров:

1. **vLLM** — OpenAI-совместимый сервер, который загружает модель строго из локального пути `/app/model` (ничего не скачивается во время запуска).
2. **FastAPI gateway** — тонкий слой над vLLM для управления сессиями, системным промптом и историей чата. Все параметры пробрасываются через переменные окружения, которые задаются в Compose.

> Внешний доступ открыт только к gateway (порт 8080). vLLM живёт в закрытой сети Compose и доступен лишь из gateway.

## Что внутри

- `app/main.py` — исходный код FastAPI сервиса с очисткой сессий, ограничением контекста и healthcheck.
- `Dockerfile` — сборка gateway.
- `docker-compose.example.yaml` — пример запуска двух контейнеров (gateway + vLLM). Пользователи копируют его в `docker-compose.yaml` и подставляют свои значения.
- `requirements.txt` — зависимости gateway.

## Требования

- Docker + Docker Compose v2.
- GPU с поддержкой CUDA для vLLM (пример использует `deploy.resources.reservations.devices`; подберите значение под свой рантайм). На CPU vLLM не работает.
- Локальная HF-совместимая модель на диске, смонтированная в контейнер vLLM по пути `./model -> /app/model`.

## Быстрый старт

1. **Подготовьте модель**
   - Скачайте модель заранее и положите в папку `./model` рядом с `docker-compose.yaml`.
   - Внутри compose папка монтируется в `/app/model` и используется и vLLM, и токенайзером gateway.

2. **Скопируйте пример compose**
   ```bash
   cp docker-compose.example.yaml docker-compose.yaml
   ```

3. **Создайте `.env` с переменными** (пример):
   ```bash
   cat > .env <<'ENV'
   # Единый ключ, который знает vLLM и который передается в /init_session
   VLLM_API_KEY=INTERNAL_VLLM_KEY

   # Максимальная длина контекста модели (используется в gateway для тримминга истории)
   MAX_CONTEXT_TOKENS=8000

   # TTL сессии в секундах при отсутствии активности
   SESSION_TTL_SECONDS=600

   # Максимальная длина контекста, которую пробрасываем в vLLM
   VLLM_MAX_MODEL_LEN=8192
   ENV
   ```

4. **Запустите**
   ```bash
   docker compose up -d --build
   ```

5. **Проверьте**
   - gateway: `curl http://localhost:8080/health`
   - vLLM остаётся без публичного порта и доступен только внутри compose.

## Переменные окружения

### Gateway
| Переменная | Назначение | Значение по умолчанию |
| --- | --- | --- |
| `VLLM_BASE_URL` | URL vLLM внутри сети Compose | `http://vllm:8000/v1` |
| `VLLM_MODEL_NAME` | Имя/путь модели для vLLM и токенайзера | `/app/model` |
| `MAX_CONTEXT_TOKENS` | Лимит входных токенов (история + запрос), gateway сам режет историю | `8000` |
| `SESSION_TTL_SECONDS` | TTL сессии без активности | `600` |

### vLLM
| Переменная | Назначение | Примечание |
| --- | --- | --- |
| `VLLM_API_KEY` | Должен совпадать с ключом, который клиенты передают в `/init_session` | Требуется для авторизации запросов |
| `VLLM_MAX_MODEL_LEN` | Пробрасывается в `--max-model-len` | Меняйте при необходимости |

> Все переменные задаются через `.env`, который автоматически подхватывает `docker compose`.

## API шлюза

Базовый URL: `http://localhost:8080`

### 1. Создать сессию
`POST /init_session`

```json
{
  "vllm_api_key": "INTERNAL_VLLM_KEY"
}
```

Ответ:
```json
{
  "session_id": "uuid",
  "expires_in": 600
}
```

### 2. Обновить системный промпт
`POST /set_system_prompt`

```json
{
  "session_id": "...",
  "system_prompt": "Ты строгий ассистент по DL"
}
```

### 3. Отправить сообщение
`POST /chat`

```json
{
  "session_id": "...",
  "message": "В чём отличие vLLM от TGI?",
  "max_tokens": 256,
  "temperature": 0.3,
  "extra": {}
}
```

Ответ содержит `reply`, `finish_reason`, счётчики токенов.

### 4. Завершить сессию
`POST /stop_session`

```json
{
  "session_id": "..."
}
```

### 5. Healthcheck
`GET /health` → `{ "status": "ok" }`

## Детали реализации

- **Сессии в памяти.** Очистка неактивных сессий каждые 60 секунд (`SESSION_TTL_SECONDS`).
- **Контекст.** Перед запросом к vLLM gateway оценивает токены с помощью локального токенайзера и удаляет самые старые пары сообщений, чтобы уложиться в `MAX_CONTEXT_TOKENS` + `max_tokens` ответа.
- **Системный промпт.** По умолчанию задаётся русскоязычный промпт; можно менять через `/set_system_prompt` в рамках сессии.
- **Авторизация.** Клиент на этапе `/init_session` отправляет `vllm_api_key`. Gateway проверяет его, делая запрос к vLLM, и выдаёт `session_id`. В дальнейших запросах используется только `session_id`; отдельного публичного токена нет.
- **Без внешних загрузок.** vLLM всегда читает модель из `/app/model`; скачивание из интернета не требуется и не производится.
- **CORS.** Включён на все источники, при необходимости ограничьте список в `app/main.py`.

## Подготовка модели

1. Предварительно скачайте модель, совместимую с Hugging Face, на хостовую машину.
2. Убедитесь, что в папке есть необходимые файлы токенайзера (gateway их читает при старте).
3. Смонтируйте папку в compose (по умолчанию `./model:/app/model:ro`).

## Обновление и обслуживание

- Для изменения параметров обновите `.env` и перезапустите `docker compose up -d`.
- Чтобы сменить модель, замените содержимое `./model` и перезапустите сервисы.
- Сессии не сохраняются между перезапусками gateway. Для персистентности интегрируйте внешний storage (Redis/Postgres).

## Отладка

- Посмотреть логи gateway: `docker compose logs -f gateway`
- Посмотреть логи vLLM: `docker compose logs -f vllm`
- Проверить здоровье сервисов: `docker compose ps`

## Лицензия

Поставьте свою лицензию при необходимости; по умолчанию репозиторий без лицензии.
