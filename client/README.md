# CLI-клиент для Simple Chat Gateway

Этот каталог содержит лёгкий консольный клиент, который общается с шлюзом FastAPI из корня репозитория. Скрипт пишет минимум кода, работает только на стандартной библиотеке и повторяет логику взаимодействия со сессиями в точности так, как это делает любой другой HTTP-клиент.

## Как устроена работа

1. **Конфигурация** хранится в JSON-файле (по умолчанию `chat.example.json`). В нём задаются хост/порт gateway, внутренний ключ `vllm_api_key`, параметры генерации (`max_tokens`, `temperature`) и дополнительные поля, которые вы хотите пробрасывать в vLLM через `extra`.
2. **Инициализация сессии** выполняется вызовом `POST /init_session` с `vllm_api_key`. Gateway проверяет ключ на стороне vLLM, создаёт в памяти новую сессию и возвращает `session_id` и TTL. Этот идентификатор хранится внутри клиента и используется во всех последующих запросах.
3. **Установка системного промпта** опциональна. Клиент предлагает взять промпт из конфигурации или ввести вручную. Запрос отправляется в `/set_system_prompt` и обновляет состояние конкретной сессии.
4. **Основной цикл общения** реализован функцией `chat_loop`. Каждое пользовательское сообщение отправляется в `/chat`, где gateway дополняет историю диалога, триммит её по лимиту `MAX_CONTEXT_TOKENS` и отправляет запрос в vLLM. Ответ, причина остановки (`finish_reason`) и счётчики токенов выводятся в консоль.
5. **Завершение работы** происходит по командам `/exit`, `/quit`, `:q` или при нажатии `Ctrl+C`. Сессия на стороне gateway очищается автоматически по TTL, но при необходимости можно вызвать `/stop_session` вручную через HTTP.

## Установка и подготовка

1. Убедитесь, что установлен Python 3.10+.
2. Перейдите в каталог `client` и при необходимости создайте виртуальное окружение:
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```
3. Установите зависимости. В файле `requirements.txt` указано, что клиент использует только стандартную библиотеку, поэтому дополнительная установка не требуется:
   ```bash
   pip install -r requirements.txt
   ```
4. Подготовьте конфигурацию:
   ```bash
   cp chat.example.json chat.json
   # отредактируйте host/port и vllm_api_key под ваш gateway
   ```

## Примеры использования

### Запуск с кастомной конфигурацией
```bash
python chat.py --config chat.json
```
При старте клиент выводит `session_id` и спрашивает, какой системный промпт использовать. Пустой ввод оставляет дефолтный промпт gateway.

### Базовые команды во время чата
- Введите любое сообщение, чтобы отправить его в модель.
- `/system <текст>` — сменить системный промпт в текущей сессии, полезно для быстрых экспериментов.
- `/exit`, `/quit` или `:q` — выйти из приложения и завершить локальный цикл.

### Пример диалога
```
$ python chat.py
Сессия создана: 9d1...eef. TTL определяется настройками gateway.
Использовать системный промпт из конфигурации? [Y/n]: y
Системный промпт установлен.
Введите сообщение. /system <текст> — сменить системный промпт, /exit — выйти.
Вы: Привет! В чём разница между vLLM и TGI?
Модель: vLLM оптимизирован под высокую пропускную способность ...
[finish_reason=stop(total_tokens=120, input_tokens=35, output_tokens=85)]
Вы: /system Ты строгий ассистент по ML.
Системный промпт обновлён.
Вы: Объясни за 1 предложение, что такое attention.
Модель: Attention — механизм ...
```

### Ручной вызов API без скрипта
Если хотите проверить шлюз напрямую, повторите шаги клиента вручную:
```bash
# 1) Создать сессию
curl -X POST http://localhost:8080/init_session \
  -H 'Content-Type: application/json' \
  -d '{"vllm_api_key": "INTERNAL_VLLM_KEY"}'

# 2) Передать session_id из ответа и установить системный промпт
curl -X POST http://localhost:8080/set_system_prompt \
  -H 'Content-Type: application/json' \
  -d '{"session_id": "<uuid>", "system_prompt": "Ты строгий ассистент"}'

# 3) Отправить сообщение
curl -X POST http://localhost:8080/chat \
  -H 'Content-Type: application/json' \
  -d '{"session_id": "<uuid>", "message": "Что такое LoRA?"}'
```

## Структура файлов
- `chat.py` — основной скрипт, реализующий HTTP-вызовы и интерактивный цикл.
- `chat.example.json` — шаблон конфигурации с полями `host`, `port`, `vllm_api_key`, `max_tokens`, `temperature`, `system_prompt`, `extra`.
- `requirements.txt` — фиксация отсутствия внешних зависимостей.

## Советы по отладке
- Если получаете `HTTP 401` при создании сессии, проверьте значение `vllm_api_key` и убедитесь, что gateway и vLLM используют один ключ.
- Ошибка подключения (`URLError`) чаще всего означает, что gateway не запущен или порт указан неверно.
- Слишком длинные ответы обрезаются по `max_tokens` и лимиту контекста gateway; при необходимости увеличьте оба значения в конфиге и переменных окружения gateway.
